\relax 
\citation{freund1995desicion}
\citation{breiman2001random}
\citation{friedman2001greedy}
\citation{buitinck2013api}
\citation{pedregosa2011scikit}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{breiman1984classification}
\@writefile{toc}{\contentsline {section}{\numberline {2}Decision Trees With Dense Input Data}{2}}
\newlabel{sec:background}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Notation}{2}}
\newlabel{gini}{{1}{2}}
\newlabel{entropy}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training}{2}}
\newlabel{red}{{3}{2}}
\newlabel{left}{{4}{2}}
\newlabel{right}{{5}{2}}
\newlabel{algo:tree-induction}{{1}{2}}
\newlabel{alg-line:partition}{{13}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The Optimal Split}{2}}
\newlabel{algo:find-best-split}{{2}{3}}
\newlabel{alg-line:value-extract}{{4}{3}}
\newlabel{alg-line:sorting}{{5}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision Trees With Sparse Input Data}{3}}
\newlabel{sec:sparse-input-dt}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sparse matrix format}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Nonzero Values of $\mathcal  {L}_p$}{3}}
\newlabel{algo:tree-induction}{{3.2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The array $mapping$ allows to efficiently compute the intersection between the $indices$ array of the csc matrix and a sample set $\mathcal  {L}_p$}}{4}}
\newlabel{fig:mapping}{{1}{4}}
\newlabel{switch}{{7}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The Optimal Split}{4}}
\newlabel{algo:sparse-split}{{3}{5}}
\newlabel{map}{{4}{5}}
\citation{joachims1996probabilistic}
\citation{bay2000archive}
\newlabel{bsearch}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}}
\newlabel{sec:experiments}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Density}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Real Data}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Leveraging the input sparsity significantly speed up decisions tree induction both with shallow and deep trees on the \emph  {20 Newsgroups} dataset. Note that the dataset is very sparse (density = 0.001).}}{6}}
\newlabel{fig:20news}{{2}{6}}
\citation{Bache+Lichman:2013}
\citation{Bache+Lichman:2013}
\citation{hastie:book2008}
\citation{breiman1984classification}
\citation{Quinlan:1993:CPM:152181}
\citation{Schapire99improvedboosting}
\citation{das:blog2014}
\citation{Zaharia:2010:SCC:1863103.1863113}
\citation{Chang:2011:LLS:1961189.1961199}
\citation{buitinck2013api}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Leveraging the input sparsity significantly speed up decisions tree induction both with shallow and deep trees on the \emph  {cup} dataset. Note that the dataset is quite sparse (density = 0.014).}}{7}}
\newlabel{fig:cup}{{3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Leveraging the input sparsity does not speed up training of deep trees on the \emph  {adult} dataset. Note that the dataset is quite dense (density = 0.11).}}{7}}
\newlabel{fig:adult}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Synthetic Data}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Related Work}{7}}
\newlabel{sec:related}{{5}{7}}
\bibstyle{abbrv}
\bibdata{references}
\bibcite{Bache+Lichman:2013}{1}
\bibcite{bay2000archive}{2}
\bibcite{breiman2001random}{3}
\bibcite{breiman1984classification}{4}
\bibcite{buitinck2013api}{5}
\bibcite{Chang:2011:LLS:1961189.1961199}{6}
\bibcite{das:blog2014}{7}
\bibcite{freund1995desicion}{8}
\bibcite{friedman2001greedy}{9}
\bibcite{hastie:book2008}{10}
\bibcite{joachims1996probabilistic}{11}
\bibcite{pedregosa2011scikit}{12}
\bibcite{Quinlan:1993:CPM:152181}{13}
\bibcite{Schapire99improvedboosting}{14}
\bibcite{Zaharia:2010:SCC:1863103.1863113}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Leveraging the input sparsity does not speed up training trees on the \emph  {tic} dataset. Note that the dataset is very dense (density = 0.44).}}{8}}
\newlabel{fig:tic}{{5}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Significant speed up is achieved by the sparsity-aware decision tree algorithm whenever the density is below 0.2 (or sparsity over 0.8).}}{8}}
\newlabel{fig:density}{{6}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}}
\newlabel{sec:conclusion}{{6}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgment}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {8}References}{8}}
